\documentclass[pdftex,a4paper,11pt]{article}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{mathtools}
\usepackage{url}



\title{Machine Learning Final Project}
\author{Michael Quigley, Zachary Wilcox}
\date{5/5/15}


\begin{document}
\maketitle
\section{Introduction}
\subsection{Problem}
Predict probabilistic distribution of hourly rain given polarimetric radar measurements.
The project attempts to use the probabilistic distribution of rain over a field using data from
polarimetric radar. Since this is a cumulative distribution, we decided that the prediction for a certain row in the test data (which is just a total amount of rain in mm for the hour) would be the center of a sigmoid, and the values of this sigmoid would be used to populate the columns in the output file. This allows people to avoid putting rain gauges everywhere. A polarimetric
radar transmits radio wave pulses in both vertical and horizontal orientations allowing the size
of a raindrop to be more easily inferred when compared with Doppler radar.
This problem is traditionally solved using some form of regression.  We have attempted to solve this problem using linear regression, an Ensemble of Decision Trees, and a binary classifier to attempt to predict each bit.

\subsection{Data}
The data contains derived quantities at a location for a period of an hour from polarimetric
radar. The data is broken up into nineteen columns: "Time To End", "Distance To Radar",
"Composite", "Hybrid Scan", "Hydrometeor Type", "Kdp", "Rain rate from HCAbased
algorithms", "Rain rate from Zdrbased
algorithms", "Rain rate from Kdpbased
algorithms",
"Radar Quality Index", "Reflectivity", "RefelectivtyQC", "RhoHV", "Velocity", "Zdrbased
Log
Water Volume", "Mass Weighted Mean", and "Mass Weighted Standard Deviation".
The columns may have more than one value. This is the case when there are multiple “Times
to End” for one radar and/or there are multiple radar readings for one location. The training
data also contains an expected amount of rain for that data location. The columns may also
contain error values. The error values are as follows: "99000",
"99901",
"99903",
"nan",
"999.0" which indicate the value for that specific element in the row is invalid.

\section{Linear Regression}
The linear regression update \cite{seber2012linear} was of the form:
$$w^{T + 1}_j = w^{T}_j - \gamma^T(w^{T} \cdot x_{i} - y_i)x_{ij}$$
Where $\gamma^T$ is the learning rate for a specific iteration, $i$ is the example number, and $j$ is the feature number. The prediction was of the form:
$$w^{T} \cdot x_{i}.$$
The learning rate was changed after every iteration using the following and was initialized as $\gamma_0$:
$$\gamma^{T + 1} = \frac{\gamma_{0}}{1 + \gamma^{T} / C}$$
Where C is a hyper parameter.




Since the columns had multiple values, the median of the column values was used as the feature value. If the value was invalid, then the median of the valid elements was used.
\subsection{Feature Selection}
The attribute 'HydrometeorType' was dropped. This was just a number that referred to a type of hydrometeor, and there was no obvious reason for this attribute. Keeping it in the set did not seem to improve accuracy by any accuracy metric. Since a distribution over the total amount of rain was the end goal, three new columns were created that were each a product of other columns. These columns roughly represented an estimate for the total amount of rain for a given amount of time. The new columns added were RR1 * TimeToEnd, RR2 * TimeToEnd, and RR3 * TimeToEnd. After this, other attributes were removed individually until accuracy was maximized. The attributes that ended up being removed for the resulting best accuracy were, 'ReflectivityQC', 'HybridScan', 'Velocity', 'Reflectivity', 'Composite', 'HydrometeorType', 'DistanceToRadar', 'Zdr', 'RhoHV', and 'Kdp'. The rain rates and radar quality index seemed to have the most influence on accuracy.




\subsection{Metrics for Accuracy}
Accuracy was measured after the first 100000 elements of the train set. The na{\"i}ve metric for accuracy that was initially used was checking to see whether the true label and the prediction were either both non-zero or both zero (0-non0). If one was zero and the other non-zero, then a mistake was counted toward the overall accuracy. This was a good initial metric as it demonstrated that the linear regression algorithm would usually predict zero when the true label was zero, and usually non zero when the true label was non zero. It could be argued that since most of the true labels were zero, a good accuracy could just be achieved by always predicting zero. What is interesting is that most of the true labels that were non zero had a non zero prediction. The problem with this metric is that many of the zero labels also had a very small non zero value. The second metric used for determining accuracy was checking to see whether the difference between the true label and the prediction was greater than some delta ($abs(a - b) > delta$). This delta was chosen to be 0.5.
\subsection{Results}
  \begin{table}[h]
        {\centering
          \begin{tabular}{|c|c|c|c|c|}
            \hline
             $\gamma_0$ & C & 0-non0 Acc. (\%) & $abs(a - b) > delta$ Acc. (\%) & Kaggle Ranking\\
            \hline
                0.0007 &   0.05        &  49.33  & 92.5 & 285/308  \\
             \hline
                0.01 &  100         &  48.2 & 86.4 & 292/308 \\
             \hline
          \end{tabular}
          \caption{Linear Regression Results}          
          \label{table:linAcc}}
  \end{table}

As stated, most of the data is a zero. One observation that was made was that most of the non-zero predictions that were very close to zero had a zero label. One addition to the third linear regression experiment was to set these small non-zero predictions to zero if they were less than some delta (0.2). The results of this addition are shown in table \ref{table:linAccModified}.
  \begin{table}[h]
        {\centering
          \begin{tabular}{|c|c|c|c|c|}
            \hline
             $\gamma_0$ & C & 0-non0 Acc. (\%) & $abs(a - b) > delta$ Acc. (\%) & Kaggle Ranking\\
            \hline
                0.0007 &   0.05        & 85.5 & 92.15 & 287/308 \\
            \hline
          \end{tabular}
          \caption{Modified Linear Regression Results}          
          \label{table:linAccModified}}
  \end{table}


\section{Adaboosted Tree Ensemble}
Both the adaBoost and decision tree algorithms were taken from the SKLearn library \cite{scikit-learn}. For learning, the labels were discretized into buckets that were ranges over an interval of size 0.3. The feature selection performed here was similar to the linear regression feature selection. The 'HydrometeorType', 'Composite', and 'Kdp' columns were removed since their removal appeared to improve accuracy. This algorithm takes too long to run on the entire train set, so a file that was just 100,000 elements of the train set was used. The tree algorithm would also discretize the input. A max tree depth of 10 was used depite there being about twice as many features. This was to mitigate overfitting. The decision criteria was based on entropy. The learning rate for adaBoost was initialized to 1.0. The first 75\% of the train file was used for learning, and the last 25\% was used for testing.
\subsection{Results}
See table \ref{table:treeEnsembleReport} for a short summary of the classification report and the other two accuracy metric results. 
  \begin{table}[h]
        {\centering
          \begin{tabular}{|c|c|}
            \hline
             Total precision & 64 \% \\
            \hline
                Precision on 0 label &   83 \% \\
             \hline
                Precision on 0.3 label &   12 \%  \\
                \hline 
                Precision on 5.4 label &   15 \%  \\
                \hline
                Precision on 10.5 label &   100 \%  \\
                  \hline
                  \hline 
                  0-non0 & 64.18 \%\\
                  \hline
                 $abs(a - b) > 0.5$ & 51.6 \% \\
                  \hline
          \end{tabular}
          \caption{Tree Ensemble Results}          
          \label{table:treeEnsembleReport}}
  \end{table}
These results were not deemed worthy of submitting to Kaggle, so we didn't submit them.
  

\section{Bit Binary Classification}
Real numbers can be represented in binary.  That means it might be possible to use a binary classifier to predict each bit of the number.  The assumption is the expected value of the data is some linear or non-linear arithmetic combination of the all the features.  Arithmetic operations can be performed at the bit level.  For example, $A$ added to $B$ at the bit level is $A \oplus B = S$, $A \land B = C$ for the sum and carry respectively \cite{lancaster2001excel}.  However, the add operation is not linearly separable because of the $\oplus$ operator.  The relation ship between all the features may also not be linear.  For these reasons we decided to use the RBF kernel which lifts it into an infinite space. This kernel is common in support vector machines (svm).  Our implementation uses a C-Support Vector Classification based of the LIBSVM library \cite{LIBSVM}\cite{scikit-learn}.\\  We used a subset of the training data with 18768 examples to train and 6256 to test. 
\subsection{Expected value representation}
The expected values are floating point with one decimal point.  IEEE 754 defines floating points to be represented with 32 bits.  We've multiplied the expected value by ten.  We then restricted the domain with eight bits.  Anything more than eight bits was rejected.  This covered all the data except for abnormally large values.
 \subsection{Best parameters}
 RBF is defined as: 
$$K(\textbf{x},\textbf{x'}) = exp(-\frac{||\textbf{x} - \textbf{x'} ||}{\gamma} )$$ where $\gamma = 2\sigma^2$\cite{vert2004primer}.  SVM is defined as:
\begin{equation}
\mathop{min}_{\textbf{w}, b, \textbf{$\xi$} } \frac{1}{2} \textbf{w}^T\textbf{w} + C\sum {\xi}
\end{equation}
subject to $y_i(\textbf{w}\phi(x_i) + b) \geq 1 - \xi_i$ \cite{LIBSVM}\\
We used a 10-fold cross validation for each bit to find the best $\gamma$ and $C$ parameter.

\begin{table}[h]
{\centering
\begin{tabular}{|c|c|c|c|c|c|c|c|c|} \hline
Bit			&	0 (LSB)	&	1	&	2	&	3	&	4	&	5	&	6	&	7(MSB) 	\\	\hline
C			&	1	&	1	&	1	&	100	&	100	&	1000	&	1	&	1		\\	\hline
$\gamma$	&	0.1	&	0.1	&	0.1	&	0.01	&	0.01	&	0.01	&	0.1	&	0.1	\\	\hline

\end{tabular}
\caption{Validation Results}          
\label{table:valresults}}
\end{table}

\subsection{Results}
The performance for each bit binary classification is quite high.  However, the precision and recall results imply that the classifiers were able to predict 0 far better than it was able to predict 1.  This is because the 80\% of the data has 0.0 for it's expected value.  One would then expect that each bit was $0$ 90\% of the time assuming the bits are uniformly distributed. 
\begin{table}[h]
{\centering
\begin{tabular}{|c|c|c|c|} \hline
	Bit 		&			precision				&		recall				&		accuracy			\\	\hline
0 (LSB)		&			0: 0.87	1: 0.00	&	0: 1.00	1:0.00	&		0.87	\\	\hline
1 				&			0: 0.89	1: 0.00	&	0: 1.00	1:0.00	&		0.89	\\	\hline
2 				&			0: 0.88	1: 0.00	&	0: 1.00	1:0.00	&		0.88	\\	\hline
3 				&			0: 0.91	1: 0.62	&	0: 0.98	1:0.28	&		0.89	\\	\hline
4 				&			0: 0.93	1: 0.62	&	0: 0.99	1:0.24	&		0.92	\\	\hline
5 				&			0: 0.96	1: 0.64	&	0: 0.99	1:0.21	&	  0.96	\\	\hline
6 				&			0: 0.98	1: 0.00	&	0: 1.00	1:0.00	&		0.98	\\	\hline
7(MSB) 		&			0: 1.00	1: 0.00	&	0: 1.00	1:0.00	&		0.99	\\	\hline
\end{tabular}
\caption{Bit Classification Results}          
\label{table:valresults}}
\end{table}

The following table contains the accuracy of the predicted expected number once the bits are reassembled relative to the true expected number:
\begin{table}[h]
{\centering 
\begin{tabular}{|c|c|c|c|} \hline
	Overall Accuracy 		&			Zero Accuracy				&		Non-Zero Accuracy		\\	\hline
		0.71							&					0.70						&				0.01					\\	\hline			
\end{tabular}
\caption{Bit Overall Results}          
\label{table:overallResults}}
\end{table}
As \ref{table:overallResults} demonstrates, the bit binary classifiers predict 0 with some degree of performance.   However, being that there only 8 bits, it did predict slightly over than $2^{-8}$.  This would suggest that, to some degree, this technique might be applied to data that was not so sparse.  That is, if the expected values or labels were more evenly distributed.  

\section{Conclusions}
The typical way to solve this problem is regression.  This seemed to perform reasonably well in comparison to the other algorithms using the selected accuracy metrics. The other methods may have worked better on data that is more uniformly distributed.

If we had more time, we would have tried more transformations such as polynomial kernels. We would have also tried more ensemble methods such as random forests. One of the most significant limiting factors was computing power. We tried running the polynomial kernels, but they took long than 48 hours and never completed. We should have used Amazon's EC2, or the HPC clusters at the University of Utah.

\bibliographystyle{plain}
\bibliography{final_bib}
\end{document}